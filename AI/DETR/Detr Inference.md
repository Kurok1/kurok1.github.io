
# 依赖需求
```plain
[accelerate](https://pypi.python.org/pypi/accelerate)==1.8.1
[aiohappyeyeballs](https://pypi.python.org/pypi/aiohappyeyeballs)==2.6.1
[aiohttp](https://pypi.python.org/pypi/aiohttp)==3.12.13
[aiosignal](https://pypi.python.org/pypi/aiosignal)==1.3.2
[annotated-types](https://pypi.python.org/pypi/annotated-types)==0.7.0
[anyio](https://pypi.python.org/pypi/anyio)==4.9.0
[async-timeout](https://pypi.python.org/pypi/async-timeout)==5.0.1
[attrs](https://pypi.python.org/pypi/attrs)==25.3.0
[beautifulsoup4](https://pypi.python.org/pypi/beautifulsoup4)==4.14.2
[certifi](https://pypi.python.org/pypi/certifi)==2025.6.15
[charset-normalizer](https://pypi.python.org/pypi/charset-normalizer)==3.4.2
[click](https://pypi.python.org/pypi/click)==8.2.1
[coloredlogs](https://pypi.python.org/pypi/coloredlogs)==15.0.1
[contourpy](https://pypi.python.org/pypi/contourpy)==1.3.2
[cycler](https://pypi.python.org/pypi/cycler)==0.12.1
[datasets](https://pypi.python.org/pypi/datasets)==3.6.0
[dill](https://pypi.python.org/pypi/dill)==0.3.8
[dnspython](https://pypi.python.org/pypi/dnspython)==2.7.0
[email_validator](https://pypi.python.org/pypi/email_validator)==2.2.0
[evaluate](https://pypi.python.org/pypi/evaluate)==0.4.4
[exceptiongroup](https://pypi.python.org/pypi/exceptiongroup)==1.3.0
[fastapi](https://pypi.python.org/pypi/fastapi)==0.115.14
[fastapi-cli](https://pypi.python.org/pypi/fastapi-cli)==0.0.7
[filelock](https://pypi.python.org/pypi/filelock)==3.13.1
[flatbuffers](https://pypi.python.org/pypi/flatbuffers)==25.9.23
[fonttools](https://pypi.python.org/pypi/fonttools)==4.58.4
[frozenlist](https://pypi.python.org/pypi/frozenlist)==1.7.0
[fsspec](https://pypi.python.org/pypi/fsspec)==2024.6.1
[gdown](https://pypi.python.org/pypi/gdown)==5.2.0
[h11](https://pypi.python.org/pypi/h11)==0.16.0
[hf-xet](https://pypi.python.org/pypi/hf-xet)==1.1.5
[httpcore](https://pypi.python.org/pypi/httpcore)==1.0.9
[httptools](https://pypi.python.org/pypi/httptools)==0.6.4
[httpx](https://pypi.python.org/pypi/httpx)==0.28.1
[huggingface-hub](https://pypi.python.org/pypi/huggingface-hub)==0.33.1
[humanfriendly](https://pypi.python.org/pypi/humanfriendly)==10.0
[idna](https://pypi.python.org/pypi/idna)==3.10
[ImageIO](https://pypi.python.org/pypi/ImageIO)==2.37.2
[imgviz](https://pypi.python.org/pypi/imgviz)==1.7.6
[Jinja2](https://pypi.python.org/pypi/Jinja2)==3.1.6
[joblib](https://pypi.python.org/pypi/joblib)==1.5.1
[kiwisolver](https://pypi.python.org/pypi/kiwisolver)==1.4.8
[lazy_loader](https://pypi.python.org/pypi/lazy_loader)==0.4
[loguru](https://pypi.python.org/pypi/loguru)==0.7.3
[markdown-it-py](https://pypi.python.org/pypi/markdown-it-py)==3.0.0
[MarkupSafe](https://pypi.python.org/pypi/MarkupSafe)==2.1.5
[matplotlib](https://pypi.python.org/pypi/matplotlib)==3.10.3
[mdurl](https://pypi.python.org/pypi/mdurl)==0.1.2
[mpmath](https://pypi.python.org/pypi/mpmath)==1.3.0
[multidict](https://pypi.python.org/pypi/multidict)==6.5.1
[multiprocess](https://pypi.python.org/pypi/multiprocess)==0.70.16
[natsort](https://pypi.python.org/pypi/natsort)==8.4.0
[networkx](https://pypi.python.org/pypi/networkx)==3.3
[numpy](https://pypi.python.org/pypi/numpy)==2.1.2
[nvidia-cublas-cu12](https://pypi.python.org/pypi/nvidia-cublas-cu12)==12.8.3.14
[nvidia-cuda-cupti-cu12](https://pypi.python.org/pypi/nvidia-cuda-cupti-cu12)==12.8.57
[nvidia-cuda-nvrtc-cu12](https://pypi.python.org/pypi/nvidia-cuda-nvrtc-cu12)==12.8.61
[nvidia-cuda-runtime-cu12](https://pypi.python.org/pypi/nvidia-cuda-runtime-cu12)==12.8.57
[nvidia-cudnn-cu12](https://pypi.python.org/pypi/nvidia-cudnn-cu12)==9.7.1.26
[nvidia-cufft-cu12](https://pypi.python.org/pypi/nvidia-cufft-cu12)==11.3.3.41
[nvidia-cufile-cu12](https://pypi.python.org/pypi/nvidia-cufile-cu12)==1.13.0.11
[nvidia-curand-cu12](https://pypi.python.org/pypi/nvidia-curand-cu12)==10.3.9.55
[nvidia-cusolver-cu12](https://pypi.python.org/pypi/nvidia-cusolver-cu12)==11.7.2.55
[nvidia-cusparse-cu12](https://pypi.python.org/pypi/nvidia-cusparse-cu12)==12.5.7.53
[nvidia-cusparselt-cu12](https://pypi.python.org/pypi/nvidia-cusparselt-cu12)==0.6.3
[nvidia-nccl-cu12](https://pypi.python.org/pypi/nvidia-nccl-cu12)==2.26.2
[nvidia-nvjitlink-cu12](https://pypi.python.org/pypi/nvidia-nvjitlink-cu12)==12.8.61
[nvidia-nvtx-cu12](https://pypi.python.org/pypi/nvidia-nvtx-cu12)==12.8.55
[onnxruntime](https://pypi.python.org/pypi/onnxruntime)==1.23.2
[opencv-python](https://pypi.python.org/pypi/opencv-python)==4.11.0.86
[osam](https://pypi.python.org/pypi/osam)==0.2.5
[packaging](https://pypi.python.org/pypi/packaging)==25.0
[pandas](https://pypi.python.org/pypi/pandas)==2.3.0
[pathlib](https://pypi.python.org/pypi/pathlib)==1.0.1
[pillow](https://pypi.python.org/pypi/pillow)==11.2.1
[pip](https://pypi.python.org/pypi/pip)==22.0.2
[propcache](https://pypi.python.org/pypi/propcache)==0.3.2
[protobuf](https://pypi.python.org/pypi/protobuf)==6.33.1
[psutil](https://pypi.python.org/pypi/psutil)==7.0.0
[pyarrow](https://pypi.python.org/pypi/pyarrow)==20.0.0
[pycocotools](https://pypi.python.org/pypi/pycocotools)==2.0.10
[pydantic](https://pypi.python.org/pypi/pydantic)==2.11.7
[pydantic_core](https://pypi.python.org/pypi/pydantic_core)==2.33.2
[Pygments](https://pypi.python.org/pypi/Pygments)==2.19.2
[pyparsing](https://pypi.python.org/pypi/pyparsing)==3.2.3
[PyQt5](https://pypi.python.org/pypi/PyQt5)==5.15.11
[PyQt5-Qt5](https://pypi.python.org/pypi/PyQt5-Qt5)==5.15.18
[PyQt5_sip](https://pypi.python.org/pypi/PyQt5_sip)==12.17.1
[PySocks](https://pypi.python.org/pypi/PySocks)==1.7.1
[python-dateutil](https://pypi.python.org/pypi/python-dateutil)==2.9.0.post0
[python-dotenv](https://pypi.python.org/pypi/python-dotenv)==1.1.1
[python-multipart](https://pypi.python.org/pypi/python-multipart)==0.0.20
[pytz](https://pypi.python.org/pypi/pytz)==2025.2
[PyYAML](https://pypi.python.org/pypi/PyYAML)==6.0.2
[regex](https://pypi.python.org/pypi/regex)==2024.11.6
[requests](https://pypi.python.org/pypi/requests)==2.32.4
[rich](https://pypi.python.org/pypi/rich)==14.0.0
[rich-toolkit](https://pypi.python.org/pypi/rich-toolkit)==0.14.7
[safetensors](https://pypi.python.org/pypi/safetensors)==0.5.3
[scikit-image](https://pypi.python.org/pypi/scikit-image)==0.25.2
[scikit-learn](https://pypi.python.org/pypi/scikit-learn)==1.7.0
[scipy](https://pypi.python.org/pypi/scipy)==1.15.3
[seqeval](https://pypi.python.org/pypi/seqeval)==1.2.2
[setuptools](https://pypi.python.org/pypi/setuptools)==59.6.0
[shellingham](https://pypi.python.org/pypi/shellingham)==1.5.4
[six](https://pypi.python.org/pypi/six)==1.17.0
[sniffio](https://pypi.python.org/pypi/sniffio)==1.3.1
[soupsieve](https://pypi.python.org/pypi/soupsieve)==2.8
[starlette](https://pypi.python.org/pypi/starlette)==0.46.2
[sympy](https://pypi.python.org/pypi/sympy)==1.13.3
[threadpoolctl](https://pypi.python.org/pypi/threadpoolctl)==3.6.0
[tifffile](https://pypi.python.org/pypi/tifffile)==2025.5.10
[timm](https://pypi.python.org/pypi/timm)==1.0.15
[tokenizers](https://pypi.python.org/pypi/tokenizers)==0.21.2
[torch](https://pypi.python.org/pypi/torch)==2.7.1+cu128
[torchvision](https://pypi.python.org/pypi/torchvision)==0.22.1+cu128
[tqdm](https://pypi.python.org/pypi/tqdm)==4.67.1
[transformers](https://pypi.python.org/pypi/transformers)==4.52.4
[triton](https://pypi.python.org/pypi/triton)==3.3.1
[typer](https://pypi.python.org/pypi/typer)==0.16.0
[typing_extensions](https://pypi.python.org/pypi/typing_extensions)==4.12.2
[typing-inspection](https://pypi.python.org/pypi/typing-inspection)==0.4.1
[tzdata](https://pypi.python.org/pypi/tzdata)==2025.2
[urllib3](https://pypi.python.org/pypi/urllib3)==2.5.0
[uvicorn](https://pypi.python.org/pypi/uvicorn)==0.34.3
[uvloop](https://pypi.python.org/pypi/uvloop)==0.21.0
[watchfiles](https://pypi.python.org/pypi/watchfiles)==1.1.0
[websockets](https://pypi.python.org/pypi/websockets)==15.0.1
[xxhash](https://pypi.python.org/pypi/xxhash)==3.5.0
[yarl](https://pypi.python.org/pypi/yarl)==1.20.1
```

# 离线推理

离线推理脚本
```python
import torch  
import torchvision.transforms as T  
from PIL import Image  
import matplotlib.pyplot as plt  
import requests  
from transformers import DetrImageProcessor, DetrForObjectDetection  
  
imageList = [  
Image.open(requests.get("https://hwpgate.xfjt.com/oss/preview/20230206-65f91278acae49d698bc813e38320350.jpg", stream=True).raw).convert("RGB") 
]  
  
# 加载预训练模型和处理器  
processor = DetrImageProcessor.from_pretrained(local_files_only=True, pretrained_model_name_or_path="/path/to/detr_model")  #指向detr模型路径
model = DetrForObjectDetection.from_pretrained(local_files_only=True, pretrained_model_name_or_path="/path/to/detr_model")  
model = model.to("mps")  #cuda 
  
for image in imageList:  
    # 图像预处理  
    inputs = processor(images=image, return_tensors="pt")  
    inputs.to("mps")  #cuda 
    # 推理  
    with torch.no_grad():  
        outputs = model(**inputs)  
  
    # 转换输出格式（获取结果）  
    target_sizes = torch.tensor([image.size[::-1]])  # PIL 是 (W,H)，要变成 (H,W)    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.1)[0]  
  
    # 可视化结果  
    draw = image.copy()  
    plt.figure(figsize=(12, 8))  
    plt.imshow(draw)  
    ax = plt.gca()  
  
    for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):  
        xmin, ymin, xmax, ymax = box.tolist()  
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,  
                                   fill=False, color="red", linewidth=2))  
        label_text = model.config.id2label[label.item()]  
        ax.text(xmin, ymin, f"{label_text}: {score:.2f}", fontsize=12,  
                bbox=dict(facecolor='yellow', alpha=0.5))  
  
    plt.axis("off")  
    plt.show()
```

# 在线推理服务构建
```python
"""
DETR online inference service (OpenAI-compatible API)
- FastAPI application
- Loads DETR models (Hugging Face Transformers / DetrForObjectDetection)
- Endpoints:
  - POST /v1/vision/detections
    Body: OpenAI-like request shape with image_url or image_base64 (base64 string)
    Supports `top_k` in request body to return only the top-k predictions per image (sorted by score desc).
  - POST /v1/vision/detect/visualize
  - GET  /v1/vision/detect/preview
- Response header `X-Prompts-Token` contains token count for the input image only
  Token formula: tokens = width * height / 784 (floating result rounded to nearest int)
Requirements:
- python 3.9+
- pip install fastapi uvicorn pillow torch transformers torchvision python-multipart requests matplotlib
Usage:
uvicorn detr_inference_service:app --host 0.0.0.0 --port 8080
"""
from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import base64
import io
from PIL import Image
import torch
from transformers import DetrForObjectDetection, DetrImageProcessor
import math
from matplotlib import pyplot as plt
from io import BytesIO
from starlette.responses import StreamingResponse

# ---------- Config ----------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ---------- FastAPI app ----------
app = FastAPI(title="DETR Inference (OpenAI-compatible)", version="0.2")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------- Request / Response Models ----------
class ImageUrlContent(BaseModel):
    url: str

class ImageInputContent(BaseModel):
    # 兼容你现有的请求结构：type=image_url 时使用 image_url.url
    # 如果后续你想更标准化，也可以改成 type=image_base64 时使用 image_base64 字段
    type: Optional[str] = None
    image_url: Optional[ImageUrlContent] = None

class ImageInputMessage(BaseModel):
    role: str = "user"
    content: List[ImageInputContent] = []

class ImageInput(BaseModel):
    model: Optional[str] = None
    messages: List[ImageInputMessage] = []
    # 新增：每张图返回 top_k 个结果（按 score 降序），不传则返回全部（受 threshold 影响）
    top_k: Optional[int] = Field(default=None, ge=1, description="Return top-k predictions per image (sorted by score desc).")

# ---------- Multi-model load ----------
MODEL_REGISTRY: Dict[str, Dict] = {
    "detr_model_name1": {
        "path": "/path/to/detr_model"
    },
    "detr_model_name2": {
        "path": "/path/to/detr_model"
    }
}

models: Dict[str, DetrForObjectDetection] = {}
processors: Dict[str, DetrImageProcessor] = {}

print("Loading DETR models...")
for name, cfg in MODEL_REGISTRY.items():
    proc = DetrImageProcessor.from_pretrained(cfg["path"], local_files_only=True)
    mdl = DetrForObjectDetection.from_pretrained(cfg["path"], local_files_only=True).to(DEVICE)
    mdl.eval()
    models[name] = mdl
    processors[name] = proc
    print(f"✅ Loaded [{name}] → {cfg['path']} on {DEVICE}")
print("All models loaded.")

# ---------- Helpers ----------
def read_image_from_base64(b64: str) -> Image.Image:
    # allow data URI or plain base64
    if b64.startswith("data:"):
        b64 = b64.split(",", 1)[1]
    try:
        decoded = base64.b64decode(b64)
        with Image.open(io.BytesIO(decoded)) as img:
            return img.convert("RGB")
    except Exception as e:
        raise ValueError(f"Invalid base64 image: {e}")

async def fetch_image_from_url(url: str) -> Image.Image:
    import requests
    r = requests.get(url, timeout=10)
    if r.status_code != 200:
        raise ValueError(f"Failed to fetch image: HTTP {r.status_code}")
    with Image.open(io.BytesIO(r.content)) as img:
        return img.convert("RGB")

def compute_image_tokens(img: Image.Image) -> int:
    w, h = img.size
    tokens = (w * h) / 784.0
    return max(1, int(math.floor(tokens + 0.5)))

def _parse_threshold_from_header(request: Request, default: float = 0.9) -> float:
    raw = request.headers.get("X-Detr-Threshold", None)
    if raw is None:
        return default
    try:
        return float(raw)
    except Exception:
        # 解析失败则回退到默认值
        return default

def run_detr_inference(
    img: Image.Image,
    model_name: str,
    threshold: float = 0.9,
    top_k: Optional[int] = None
) -> List[Dict[str, Any]]:
    if model_name not in models:
        raise ValueError(f"Model '{model_name}' not loaded. Available models: {list(models.keys())}")

    model = models[model_name]
    processor = processors[model_name]

    inputs = processor(images=img, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)

    target_sizes = torch.tensor([img.size[::-1]], device=DEVICE)  # (h, w)
    results = processor.post_process_object_detection(
        outputs,
        threshold=threshold,
        target_sizes=target_sizes
    )[0]

    preds: List[Dict[str, Any]] = []
    for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
        label_id = int(label.detach().cpu().item())
        preds.append({
            "score": float(score.detach().cpu().item()),
            "label_id": label_id,
            "label": model.config.id2label[label_id] if hasattr(model.config, "id2label") else str(label_id),
            "bbox": [float(x) for x in box.detach().cpu().tolist()]
        })

    # 关键：top-k（每张图内按分数排序后截断）
    preds.sort(key=lambda x: x["score"], reverse=True)
    if top_k is not None:
        preds = preds[:top_k]

    # 释放显存（如果在 cuda 上）
    del inputs
    del outputs
    if DEVICE == "cuda":
        torch.cuda.empty_cache()

    return preds

# ---------- Endpoint ----------
@app.get("/v1/vision/detect/preview")
async def predict_preview(model: str, url: str, threshold: float = 0.9, top_k: int = 10):
    # 该接口是 GET，无 request body；如你也希望 preview 支持 top_k，可以加 query 参数 top_k
    try:
        image = await fetch_image_from_url(url)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

    results = run_detr_inference(image, model, float(threshold), top_k=top_k)

    draw = image.copy()
    plt.figure(figsize=(12, 8))
    plt.imshow(draw)
    ax = plt.gca()
    for result in results:
        xmin, ymin, xmax, ymax = result["bbox"]
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                   fill=False, color="red", linewidth=2))
        ax.text(xmin, ymin, f'{result["label"]}: {result["score"]:.2f}', fontsize=12,
                bbox=dict(facecolor="yellow", alpha=0.5))
    plt.axis("off")

    buf = BytesIO()
    plt.savefig(buf, format="jpg")
    buf.seek(0)
    plt.close()
    return StreamingResponse(buf, media_type="image/jpeg")

@app.post("/v1/vision/detect/visualize")
async def predict_visualize(req: ImageInput, request: Request, response: Response):
    if not req.messages or not req.messages[-1].content:
        raise HTTPException(status_code=400, detail="Missing messages/content")

    # 这里沿用你原来的取法：取最后一个 content 做可视化
    content = req.messages[-1].content[-1]
    image = None

    try:
        if content.type == "image_url" and content.image_url:
            image = await fetch_image_from_url(content.image_url.url)
        elif content.type == "image_base64" and content.image_url:
            # 兼容：你目前把 base64 也塞在 image_url.url 里
            image = read_image_from_base64(content.image_url.url)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

    if image is None:
        raise HTTPException(status_code=400, detail="Invalid input type or image payload")

    model_name = request.headers.get("X-Model", req.model) or req.model
    threshold = _parse_threshold_from_header(request, default=0.9)
    top_k = req.top_k

    results = run_detr_inference(image, model_name, threshold=threshold, top_k=top_k)

    draw = image.copy()
    plt.figure(figsize=(12, 8))
    plt.imshow(draw)
    ax = plt.gca()
    for result in results:
        xmin, ymin, xmax, ymax = result["bbox"]
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                   fill=False, color="red", linewidth=2))
        ax.text(xmin, ymin, f'{result["label"]}: {result["score"]:.2f}', fontsize=12,
                bbox=dict(facecolor="yellow", alpha=0.5))
    plt.axis("off")

    buf = BytesIO()
    plt.savefig(buf, format="jpg")
    buf.seek(0)
    plt.close()
    return StreamingResponse(buf, media_type="image/jpeg")

@app.post("/v1/vision/detections")
async def detections(req: ImageInput, request: Request, response: Response):
    """OpenAI-like endpoint for DETR detection.
    Accepts image_url or image_base64 (base64 currently carried in image_url.url for compatibility).
    Returns predictions and sets X-Prompts-Token header.
    Supports `top_k` in request body to return only top-k predictions per image.
    """
    if not req.messages or not req.messages[-1].content:
        raise HTTPException(status_code=400, detail="Missing messages/content")

    image_list: List[Image.Image] = []
    for content in req.messages[-1].content:
        try:
            if content.type == "image_url" and content.image_url:
                img = await fetch_image_from_url(content.image_url.url)
                image_list.append(img)
            elif content.type == "image_base64" and content.image_url:
                img = read_image_from_base64(content.image_url.url)
                image_list.append(img)
        except Exception as e:
            raise HTTPException(status_code=400, detail=str(e))

    if not image_list:
        raise HTTPException(status_code=400, detail="No valid images found in request")

    prompts_tokens = 0
    model_name = request.headers.get("X-Model", req.model) or req.model
    threshold = _parse_threshold_from_header(request, default=0.9)
    top_k = req.top_k

    res: List[Dict[str, Any]] = []
    try:
        for index, img in enumerate(image_list):
            prompts_tokens += compute_image_tokens(img)
            preds = run_detr_inference(img, model_name, threshold=threshold, top_k=top_k)
            res.append({
                "index": index,
                "result": preds
            })

        response.headers["X-Prompts-Token"] = str(prompts_tokens)
        return {
            "object": "detection",
            "model": model_name,
            "top_k": top_k,          # 便于调用方确认服务端实际使用的 top_k
            "threshold": threshold,  # 同上
            "predictions": res
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Model inference error: {e}")

# models list
@app.get("/models")
def list_models():
    return {"loaded_models": list(models.keys())}

# health
@app.get("/health")
def health():
    return {"status": "ok", "model": MODEL_REGISTRY, "device": DEVICE}

# ---------- Example cURL ----------
# curl --location 'http://${host}:${port}/v1/vision/detections' \
# --header 'X-Model: detr_model_name1' \
# --header 'X-Detr-Threshold: 0.5' \
# --header 'Content-Type: application/json' \
# --data '{
#   "top_k": 5,
#   "messages": [
#     {
#       "role": "user",
#       "content": [
#         {
#           "type": "image_url",
#           "image_url": { "url": "https://66yunlian-res-public-prod.oss-cn-beijing.aliyuncs.com/prod/nywl/20250812/953196f26488411f91371eb2580ea5fe.jpg" }
#         }
#       ]
#     }
#   ]
# }'
# curl http://${host}:${port}/v1/vision/detect/preview?model=detr-resnet-50_staff_20250726&url=https://66yunlian-res-public-prod.oss-cn-beijing.aliyuncs.com/prod/nywl/20250812/953196f26488411f91371eb2580ea5fe.jpg
```
