- **先搞懂 MLP 的前向/反向、激活函数、损失函数** 
[参考连接](https://zh.d2l.ai/chapter_preface/index.html)
    
- **理解“序列”为什么需要特殊结构**（变长、顺序、上下文）
    
- 学 **RNN → LSTM/GRU**：重点理解“隐藏状态”的意义
    
- 学 **Seq2Seq**：编码器/解码器怎么生成
    
- 学 **Attention**：为什么能缓解瓶颈、怎么做“对齐”
    
- 学 **Self-Attention**：Q/K/V 的直觉（“我在找什么/我有什么/我要取什么”）
    
- 最后拼起来就是 **Transformer**：
    
    - 编码器层结构
        
    - 解码器层结构（mask 很关键）
        
    - 位置编码的必要性

[从神经网络到Transformers](./从神经网络到Transformers.md)
