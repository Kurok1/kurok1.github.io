# 什么是模型量化
模型量化（Model Quantization）是把神经网络中原本以高精度表示的数值（主要是**权重**和/或**激活值**，有时也包括梯度/优化器状态）转换为更低位宽的数值表示（例如从 FP32/FP16 转为 INT8/INT4/FP8 等），以换取更小的模型体积、更低的内存带宽占用、更快的推理速度，以及更低的功耗/成本。

下面按“是什么—怎么做—有什么区别/代价”把关键点说明清楚。

---

## 1) 量化到底在改什么

典型的深度学习模型推理涉及大量矩阵乘法与张量读写。量化主要做两件事：

1. **降低表示精度**
    
    - 权重：FP16/FP32 → INT8/INT4 等
        
    - 激活：FP16/FP32 → INT8 等（更激进也可到 INT4/FP8）
        
2. **用缩放因子（scale）把浮点映射到整数**  
    常见形式是：  
    $$  
    x \approx s \cdot q  
    $$
    其中 (q) 是整数（例如 INT8），(s) 是浮点缩放因子；也可能带零点（zero-point）做非对称映射。
    

直观理解：用更少的“刻度”去表示数值，存储和计算更省，但会引入“舍入误差”。

---

## 2) 常见量化类型（你会在工程里遇到的）

### A. 按“是否需要数据/训练”分

- **PTQ（Post-Training Quantization，训练后量化）**  
    不再训练或只用少量校准数据（calibration）统计激活分布，然后把权重/激活量化。优点是简单、快；缺点是对精度更敏感，尤其是低比特（如 INT4）。
    
- **QAT（Quantization-Aware Training，量化感知训练）**  
    训练时就模拟量化误差，让模型“适应”低精度。通常能比 PTQ 保留更多精度，代价是训练复杂度和成本更高。
    

### B. 按“量化对象”分

- **仅权重量化（Weight-only quantization）**  
    激活仍用 FP16/BF16，权重用 INT8/INT4。大模型推理中非常常见：带宽/显存压力显著下降，精度通常更稳。
    
- **权重+激活量化（W/A quantization）**  
    进一步加速，但需要硬件与算子更好支持，精度风险更高、实现更复杂。
    

### C. 按“缩放粒度”分

- **per-tensor**：整个张量一个 scale（简单但误差可能更大）
    
- **per-channel（常见）**：每个输出通道一个 scale（精度更好，尤其对卷积/线性层）
    
- **per-group / blockwise（大模型 INT4 常见）**：每组若干权重共享 scale，兼顾精度与存储/吞吐
    

---

## 3) 量化后的模型与原始模型的主要区别

### 3.1 体积与显存占用

- 权重从 FP16（2 字节）到 INT8（1 字节）≈ **减半**
    
- 权重到 INT4（0.5 字节）≈ **再减半**（相对 FP16 约 1/4）
    
- 若从 FP32 到 INT8/INT4，压缩更明显
    

这直接影响：

- 能否在同一张卡上放下更大模型/更长上下文
    
- batch size、并发数、吞吐能力
    

### 3.2 推理速度与吞吐

加速来源通常不是“算力”本身，而是：

- **内存带宽压力降低**（读权重更快）
    
- **缓存命中率更高**
    
- 硬件（GPU/CPU/NPU）对 INT8/INT4/FP8 的矩阵乘支持
    

注意：是否真的加速取决于**硬件支持 + 内核实现 + 量化形式**。比如“权重 INT4，但计算时需要反量化到 FP16”会削弱收益；而成熟的 INT8/FP8 张量核或高效的 INT4 GEMM 内核通常收益更大。

### 3.3 数值行为与精度差异（最关键的代价）

量化引入误差，表现为：

- **困惑度（Perplexity）上升**（语言模型）
    
- **准确率/召回率下降**（分类/检索）
    
- **输出更不稳定**：同一 prompt 可能更容易出现偏差、格式不稳、长链推理更易“跑偏”
    
- **对“极端值/长尾分布”敏感**：某些层（如 attention 的投影、输出层、归一化附近）对量化更脆弱
    

经验上（不把它当硬规则）：

- INT8（尤其 per-channel）通常精度损失很小
    
- INT4 更依赖优秀的算法（groupwise、GPTQ/AWQ 等）与校准数据；不同任务差异较大
    
- 激活也量化（W/A）比仅权重更容易带来可见精度损失
    

### 3.4 运行方式与工程形态

量化后你往往会看到这些变化：

- 模型文件格式不同（多了 scale/zero-point、分组信息）
    
- 推理图中多了量化/反量化算子（Q/DQ），或直接走整数内核
    
- 部分算子可能回退到高精度（“混合精度/混合量化”）以保精度
    
- 可训练性不同：很多部署量化模型主要用于推理；若要继续训练，通常需要 QAT 或特殊策略
    

### 3.5 可移植性与兼容性

- 某些量化模型依赖特定推理引擎/内核（例如某些 GPU 内核、特定 runtime）
    
- 不同平台对 INT4/INT8/FP8 支持差异很大（CPU、不同代 GPU、移动端 NPU）
    

---

## 4) 什么时候该量化，什么时候不该

### 值得量化的典型场景

- 部署成本敏感：显存/内存/带宽/功耗是瓶颈
    
- 需要更高吞吐或更低延迟
    
- 想在同一硬件上跑更大模型或更长上下文
    
- 对微小精度下降可容忍（或可通过评测验证影响可控）
    

### 谨慎量化的场景

- 对精度极其敏感（例如某些合规、医疗、金融关键决策）
    
- 输出必须非常稳定、格式严格（虽然可以通过部分层保留高精度缓解）
    
- 你没有足够的校准数据/代表性输入来做 PTQ 校准
    
- 目标硬件/推理引擎对你选的量化形态支持不成熟
    

---

## 5) 一个简短结论（便于记忆）

量化本质是“**用更少的比特表示权重/激活，换取更低成本与更高速度**”。  
量化后模型通常：

- **更小、更省显存、可能更快**
    
- 但会引入**数值误差**，导致**精度/稳定性可能下降**，并带来**平台/内核依赖**和**工程复杂度**。
    

---

# 有哪些具体的量化方法？
量化方法可以按“量化发生在何时、量化到什么精度、量化哪些张量、用什么映射规则、以及针对大模型的特定算法”来分类。下面给出工程上最常见、也最有用的一套分类清单，并说明各自适用场景与取舍。

---

## 1) 按流程：训练后量化 vs 量化感知训练

### PTQ：Post-Training Quantization（训练后量化）

不再训练（或只做少量校准），直接把已训练模型量化。

- **PTQ-Static（静态/校准量化）**：用校准数据统计激活分布，确定 scale/zero-point，再量化权重与激活。
    
- **PTQ-Dynamic（动态量化）**：推理时对激活按 batch/句子动态计算 scale；权重通常预量化。常见于 CPU 推理。
    

优点：实现快、成本低；缺点：低比特（如 INT4）更容易掉精度，需要更好的校准与算法。

### QAT：Quantization-Aware Training（量化感知训练）

训练过程中插入“伪量化”（fake quantization）模拟舍入/截断误差，使参数适应量化。

- 通常精度最好（尤其 W/A 量化、较低比特）。
    
- 成本：训练复杂、需要更长训练/更谨慎的超参。
    

---

## 2) 按对象：权重量化 / 激活量化 / 混合量化

### 权重-only 量化（Weight-only）

- 权重 INT8/INT4，激活保持 FP16/BF16。
    
- LLM 部署里非常常见（带宽/显存收益显著，精度风险较小）。
    

### 权重 + 激活量化（W/A Quant）

- 权重和激活都量化（如 INT8×INT8，或 FP8 等）。
    
- 推理速度潜力更大，但更依赖硬件与内核，精度更敏感。
    

### 混合精度/混合位宽（Mixed Precision / Mixed Bit）

- 关键层（如 embedding、输出层、部分 attention 投影）保留高精度，其余层量化。
    
- 常用来“保精度 + 降成本”折中。
    

---

## 3) 按数值格式：INT8/INT4/FP8/二值等

- **INT8**：工业界最成熟，硬件支持广，通常易于保持精度。
    
- **INT4 / 4-bit**：压缩与带宽收益更大，但精度更敏感；常配合“按组/分块量化 + 误差补偿”算法。
    
- **FP8（E4M3/E5M2 等）**：更像“低精度浮点”，训练与推理都有应用，依赖新硬件与栈支持。
    
- **Binary/Ternary（1-bit/2-bit）**：研究和特定场景有价值，但通用大模型部署较少。
    

---

## 4) 按映射规则：对称/非对称、scale 粒度、舍入与截断策略

### 对称 vs 非对称

- **对称（symmetric）**：零点固定为 0，映射简单，常用于权重。
    
- **非对称（asymmetric）**：带 zero-point，更适合偏移明显的分布（常用于激活）。
    

### scale 粒度（非常关键）

- **per-tensor**：整个张量一个 scale（快、简单，精度可能差）。
    
- **per-channel**：每个输出通道一个 scale（精度更好，常用）。
    
- **per-group / blockwise**：每组若干参数共享 scale（LLM 4bit 常见，平衡精度与开销）。
    

### 截断/裁剪（clipping）与标定（calibration）

- 用 min/max、百分位（percentile）、KL 散度、MSE 最小化等方法决定量化范围，减少离群值影响。
    

---

## 5) 按是否“只量化存储，计算仍用高精度”：这决定性能形态

### 反量化计算（dequantize-then-matmul）

- 权重存 INT4/INT8，但 GEMM 前解码/反量化到 FP16 再算。
    
- 实现相对容易，速度收益不一定最大（但能显著省显存）。
    

### 真整数/低精度核（int8/int4 kernels 或 fp8 kernels）

- 直接用低精度张量核/整数核计算，速度收益更显著，但依赖硬件与内核成熟度。
    

---

## 6) 面向大模型（LLM）的代表性 4/8-bit PTQ 算法家族

这类方法通常属于“训练后量化”，核心是：**更聪明的分组量化 + 误差补偿/重要性度量**。

### GPTQ（Gradient Post-Training Quantization）

- 基于二阶信息/近似 Hessian 的逐层/逐列量化，尽量最小化量化误差对输出的影响。
    
- 精度通常很好，但量化过程可能更慢、更复杂。
    

### AWQ（Activation-aware Weight Quantization）

- 利用激活统计信息“感知”哪些权重更重要，通过缩放/重参数化让关键权重更好量化。
    
- 量化速度和部署生态通常较友好，实践中很常见。
    

### SmoothQuant

- 核心思想：把激活中的“难量化尺度”通过缩放迁移到权重端，改善 W/A INT8 的可量化性。
    
- 常用于激活也要 INT8 的场景。
    

### RPTQ / ZeroQuant / 等变体

- 各自围绕校准策略、分组、误差传播、在线/离线策略做改进；工程上常以“某推理引擎支持的方案”为准。
    

---

## 7) 其它常用工程技巧（严格说是“量化方法组合件”）

- **偏置（bias）保持高精度**：常见做法，避免误差放大。
    
- **分层回退（fallback）**：某些层不用 INT4，用 INT8/FP16。
    
- **双量化/二级量化（double quant）**：连 scale 也压缩，进一步省显存（常见于 LLM 4bit 存储格式）。
    
- **校准数据选择**：尽量贴近真实分布（业务 prompt/输入），否则 PTQ 容易失真。
    
- **度量驱动**：用任务指标/困惑度/一致性测试决定哪些层回退、用什么 group size。
    

---

## 8) 一个简明选型指南（实务导向）

- **优先稳妥**：INT8（per-channel）权重或 W/A INT8（有成熟内核时）。
    
- **显存最敏感**：4-bit weight-only（优先选成熟的 GPTQ/AWQ 生态），关键层必要时回退到 8/16-bit。
    
- **追求极致吞吐**：如果硬件/框架支持好，考虑 FP8 或 INT8×INT8 的算子路径。
    
- **要训练或持续微调**：倾向 QAT 或者“量化友好”的训练策略，否则 PTQ 可能不稳。
    
