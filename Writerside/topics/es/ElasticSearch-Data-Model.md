# ElasticSearch数据副本模型

我们知道，ES的索引分配分为主分片和从分片（副本），因此需要考虑以下几个问题：

* 主从同步机制
* 从分片如何恢复主副本数据
* 主分片下线了，如何保持正常工作
* 多从分片存在的场景下，是否要全部分片写入成功才会返回给客户端



## 1.模型引入：

在解决问题之前，先简单描述下ElasticSearch的数据模型

![数据模型](elasticsearch-data-model.png)



* Master负责维护索引的元信息
* 集群状态（使用 GET /_cluster/state 可以查询集群状态）中的 routing_table 保存了所有索引相关的信息，包括索引的分片、分片所在的节点等信息。而其中 **Index Metadata 中的 in_sync_allocations 记录了同步的副本**
* ES 为每次写入操作都分配一个唯一的 ID，也就是 **Sequence Number 来记录每个写入操作** 
* 在每次 Primary 变更的时候都会对主分片做一个标记（变更时自增），叫做 **Term（任期）** 。可以看成类似版本的概念
* ES 使用 **Checkpoint** 来保存每次写入操作的位置，后续数据恢复也是根据**Checkpoint**来恢复



## 2.选择主分片

我们知道，主分片保证了数据的一致性，如果出现双主分片，那么会出现数据不一致的情况，因此在集群中，要避免双主分片的出现



主分片的选择是由Master进行的。在系统启动的时候，由于没有分片的信息，所以 **Master 会向集群中所有数据节点询问这个分片的元信息**。在 Master 获取了分片的元信息后，接下来就要在多个符合要求的副本中选取一个作为主副本了。



ES 在 5.x 之后的版本中引入了 Alloction ID 来作为每个副本的唯一标识。在集群的元信息中，**in_sync_allocations（同步副本集合）** 记录相关分片活跃的、含有最新数据的副本的 Alloction ID。在系统启动时，主节点会检查副本的 Allocation ID 是否在 in_sync_allocation 中，**只有存在于这个集合中，这个副本才有可能被选为主副本。**



## 3.主从数据同步

现在我们知道了主分片是如何产生的，现在来看看是如何保证主从数据一致的。

文档分布式存储首先需要找到能存储文档的主分片，并在主分片的节点上写入对应数据，**数据在主分片写入成功后再将数据分发到副分片进行存储**。文档的新增、更新、删除等操作都属于写入操作。

比如我们目前有三个节点，node1，node2，node3。node1为主分片节点

1. 数据写入操作请求到master，master转发至node1

2. node1写入完成后，操作写入**transacton log**

3. 转发请求至node2，执行相同操作

4. 转发请求至node3，执行相同操作

注意，ElasticeSearch是一个非实时性系统，因此必然无法保证数据的强一致性。在主分片node1写入**transaction log**，后，即可视为写入完成成功，此时反馈客户端

而数据的可靠性，则是通过transaction log来保证的，这点类似于mysql中的binlog机制



## 4.写故障处理

写故障分为两类故障：主副本写入故障和从副本写入故障

### 4.1 主副本写入故障

如果**写主分片发生错误**，那么这个主分片所在的节点就会向 Master 汇报。这个时候写入操作会等待（默认 1 分钟）直到 Master 选出一个新的主副本，然后写入操作会被转发到新的副本中进行处理。如果是主副本因为网络异常、宕机离线的话，Master 也会自动监测节点的健康状态，并且会主动降级某个离线主副本。



### 4.2 从副本写入故障

如果在**写入其他从副本时发生异常**，主副本节点会通知 Master 将这个异常的副本从同步副本集合（in-sync_replica_set）中移除。一旦 Master 确认移除这个副本，主副本就会确认这次操作。需要注意的是，Master 会在另一个节点上创建一个新的副本来使集群状态变为健康状态。



## 5.读故障处理

当一个副本无法响应请求的时候，协调节点会将请求转发给同一个副本组的另一个副本进行处理，重复的失败可能导致副本组中所有的副本都被移除，从而没有副本可用。为了快速响应，像 Search、Multi Search、Bulk、Multi Get 这些接口在发生失败时都只会返回部分结果。



## 6.数据恢复

如果所有从副本都下线了（磁盘挂了，数据无法找回），并且没有在其他节点上进行恢复，那么如果主副本也挂了，就可能造成数据丢失。所以，当系统中有部分从副本下线后，系统需要在其他节点中恢复这个副本。

而当系统需要恢复一个副本的时候，需要保证恢复后的数据跟主分片一致。一般数据恢复有两种方式：数据文件全量复制和增量复制。

对于**文件全量复制**来说，如果数据量很大的时候，耗时会非常长，在将副本分配给新节点的时候就要进行全量复制。而要实现**增量复制**必须知道从副本与主副本间的差异，不然两个副本全量数据进行比较也是吃不消的。

ES 使用本地检查点和全局检查点来标记从副本与主副本的差异，而这两个检查点的值就是上述提到的 Sequence Number。

- **全局检查点（GlobalCheckpoint）**，是所有活跃分片历史都已经对齐、持久化成功的序列号，所以**小于全局检查点的操作都已经在所有副本上处理完了**。当主副本下线后，系统只需要比较新的主副本与其他从副本间最后一个全局检查点之后的操作即可。
- **本地检查点（LocalCheckpoint）**，代表着**本副本中所有小于这个值的操作都已经处理完毕了**（写 Lucene 和 Translog 都成功了）。

有了全局检查点后，系统就可以实现增量数据复制了，系统只需要比较副本间最后一个全局检查点即可知道差异的数据，并且对其增量复制。由于篇幅的限制，数据恢复的细节内容不在文中的讨论范围内。

但是，如果很不幸系统发生了严重的灾难，集群中只有旧的副本，由于这些旧的副本不在 in-sync-allocation IDs 集合中，所以系统无法自动进行主分片分配，只能进行手工恢复。

这时候该怎么办呢？可以**使用 allocate_stale_primary 将一个指定的旧分片分配为主分片！** 如下所示：

```bash
POST _cluster/reroute
{
  "commands" : [
    {
      "allocate_stale_primary" : {
          "index" : "xxxx", 
          "shard" : 2,
          "node" : "2222",
          "accept_data_loss":true
      }
    }
  ]
}
```

但是，需要强调的是：将旧数据的分片分配为主分片，会造成部分数据丢失！如果 in_sync_allocations 中的同步分片只是暂时不可用的话，对于这条命令请谨慎使用。